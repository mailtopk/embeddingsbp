{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP+NO9iuMVTJ0kcI/jRL1jL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mailtopk/embeddingsbp/blob/main/Embeddings_bestPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKFnAwZ3vYsP",
        "outputId": "6c9a5c7b-c0d2-4ce3-950b-b523640a69e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Aug 31 01:28:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0             31W /   70W |    2498MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6CIolI42QP7"
      },
      "outputs": [],
      "source": [
        "#Setup\n",
        "%pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Understand embeddings layer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerEmbeddings(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, max_length=512):\n",
        "    super.__init__()\n",
        "\n",
        "    # Token embeddings: convert words to vectors\n",
        "    self.token_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Positional embeddings: add sequence order information\n",
        "    self.positional_embeddings = nn.Embedding(max_length, embedding_dim)\n",
        "\n",
        "    # Segment embeddings: distinguish different parts of input (e.g., question vs context)\n",
        "    self.segment_embeddings = nn.Embedding(2, embedding_dim)\n",
        "\n",
        "    # Layer normalization\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, input_ids, segment_id=None):\n",
        "    # Get sequance length from input\n",
        "    seq_length = input_ids.size(1)\n",
        "\n",
        "    # Create position IDs indices (0, 1, 2,..)\n",
        "    position_ids = torch.arange(seq_length, device=input_ids.device)\n",
        "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "    #segment embedding, if non initialized to zero\n",
        "    if segment_id is None:\n",
        "      segment_id = torch.zeros_like(input_ids)\n",
        "\n",
        "    # combine all embedings\n",
        "    embeddings = (\n",
        "        self.token_embeddings(input_ids) +\n",
        "        self.positional_embeddings(position_ids) +\n",
        "        self.sengment_embeddings(segment_id)\n",
        "    )\n",
        "\n",
        "    # Apply layer normalization and dropout\n",
        "    embeddings = self.layer_norm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enP1zmUsZsJ9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "monolingual_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "multilingual_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
        "\n"
      ],
      "metadata": {
        "id": "7hX8xeZ12YB2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example sentences in different languages\n",
        "sentences = {\n",
        "    'english': \"The weather is beautiful today.\",\n",
        "    'spanish': \"El clima está hermoso hoy.\",\n",
        "    'french': \"Le temps est magnifique aujourd'hui.\",\n",
        "    'german': \"Das Wetter ist heute wunderschön.\"\n",
        "}\n",
        "\n",
        "# Function to compute similarity between embeddings\n",
        "def compute_similarity(emb1, emb2):\n",
        "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "# Compare embeddings across languages\n",
        "def compare_sentences(model, sentences):\n",
        "    # Generate embeddings for all sentences\n",
        "    embeddings = {\n",
        "        lang: model.encode(text, convert_to_numpy=True)\n",
        "        for lang, text in sentences.items()\n",
        "    }\n",
        "\n",
        "    # Compare each pair\n",
        "    print(f\"\\nSimilarity scores for {model.__class__.__name__}:\")\n",
        "    for lang1 in sentences:\n",
        "        for lang2 in sentences:\n",
        "            if lang1 < lang2:  # avoid duplicate comparisons\n",
        "                sim = compute_similarity(embeddings[lang1], embeddings[lang2])\n",
        "                print(f\"{lang1} vs {lang2}: {sim:.4f}\")"
      ],
      "metadata": {
        "id": "w_JkoMDq3QXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "multilingual_model = multilingual_model.to(device)\n",
        "monolingual_model = monolingual_model.to(device)\n",
        "\n",
        "# Test device\n",
        "monolingual_model = monolingual_model\n",
        "for model in [monolingual_model, multilingual_model]:\n",
        "    compare_sentences(model, sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuyrYTmf4SsZ",
        "outputId": "a90717db-d00c-4486-e4c2-950ec54a4631"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similarity scores for SentenceTransformer:\n",
            "english vs spanish: 0.1565\n",
            "english vs french: 0.1190\n",
            "english vs german: 0.1472\n",
            "french vs spanish: 0.3038\n",
            "french vs german: 0.2329\n",
            "german vs spanish: 0.0404\n",
            "\n",
            "Similarity scores for SentenceTransformer:\n",
            "english vs spanish: 0.9916\n",
            "english vs french: 0.9766\n",
            "english vs german: 0.9900\n",
            "french vs spanish: 0.9801\n",
            "french vs german: 0.9930\n",
            "german vs spanish: 0.9877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of batch processing for efficiency\n",
        "texts = list(sentences.values())\n",
        "batch_embeddings = multilingual_model.encode(texts, batch_size=8)\n",
        "batch_embeddings"
      ],
      "metadata": {
        "id": "hdMCKVRR4ZU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5c9e7b-922b-4bdb-eef6-f609f3719982"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.35319242, -0.08173984, -0.01260098, ...,  0.0598221 ,\n",
              "         0.11287276,  0.06665527],\n",
              "       [-0.34583592, -0.09804583, -0.01258651, ...,  0.0550659 ,\n",
              "         0.1120597 ,  0.07015114],\n",
              "       [-0.3260478 , -0.14656122, -0.01337153, ...,  0.06969073,\n",
              "         0.11693197,  0.08681154],\n",
              "       [-0.34187278, -0.1172059 , -0.01355953, ...,  0.05922513,\n",
              "         0.11434709,  0.07827169]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Length Handling: Be aware of maximum sequence length\n",
        "\n",
        "max_seq_length = model.max_seq_length\n",
        "def chunk_text(text, max_length=max_seq_length):\n",
        "    # Split into sentences or chunks\n",
        "    chunks = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Process long document\n",
        "long_text_embeddings = model.encode(chunk_text(long_document))"
      ],
      "metadata": {
        "id": "zhdmoebJ5m9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization: Consider normalizing embeddings for cosine similarity\n",
        "# Makes cosine similarity calculations more efficient\n",
        "# Ensures consistent similarity scores across different lengths of text\n",
        "# Reduces the impact of text length on similarity calculations\n",
        "# Important for comparing embeddings across different documents\n",
        "\n",
        "embeddings = model.encode(list(sentences), normalize_embeddings=True)\n",
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCJKYzu1rz__",
        "outputId": "7905abe7-691d-4bca-ed36-eaac231472ac"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.04567016,  0.00326327, -0.00472475, ...,  0.01196525,\n",
              "        -0.00940966, -0.03085416],\n",
              "       [-0.06772514, -0.01382657, -0.00457956, ...,  0.00126622,\n",
              "         0.02872803, -0.000449  ],\n",
              "       [-0.02946393,  0.02743342, -0.00471895, ...,  0.00393686,\n",
              "         0.06189221, -0.01214308],\n",
              "       [-0.06218855, -0.03281022, -0.00310089, ..., -0.04045001,\n",
              "         0.04895798, -0.04151825]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory management\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def process_large_dataset(sentences_dict, batch_size=32):\n",
        "\n",
        "  if sentences_dict is None:\n",
        "    return None\n",
        "\n",
        "  if type(sentences_dict) is not dict:\n",
        "    raise ValueError(\"Input must be a dictionary\")\n",
        "\n",
        "  embeddings = []\n",
        "  sentences = list(sentences_dict.values()) # Get the values as a list\n",
        "  num_sentences = len(sentences)\n",
        "\n",
        "  for i in range(0, num_sentences, batch_size):\n",
        "    batch_sentences = sentences[i:i+batch_size]\n",
        "    batch_embeddings = model.encode(batch_sentences)\n",
        "    embeddings.extend(batch_embeddings)\n",
        "\n",
        "    # clear GPU\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    gc.collect()\n",
        "  return embeddings\n",
        "\n",
        "process_large_dataset(sentences, batch_size=2)"
      ],
      "metadata": {
        "id": "WeFRZnDfsO6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error handling\n"
      ],
      "metadata": {
        "id": "4v_LAfzi5C6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero Short Learning"
      ],
      "metadata": {
        "id": "NhApz56E-GRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "class ZeroShortClassification:\n",
        "  def __init__(self, model_name=\"all-mpnet-base-v2\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.model = SentenceTransformer(model_name, device=device)\n",
        "\n",
        "  def classify(self, text, labels):\n",
        "    text_embedding = self.model.encode(text, convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "    # Prepair label prompt\n",
        "    label_prompts = [f\"This text is about label {label}\" for label in labels]\n",
        "    print(label_prompts)\n",
        "\n",
        "    # Encode label prompts\n",
        "    label_embedding = self.model.encode(label_prompts, convert_to_tensor=True, normalize_embeddings=True)\n",
        "\n",
        "    # calculate similarities\n",
        "    similarities = util.pytorch_cos_sim(text_embedding, label_embedding)[0]\n",
        "\n",
        "    results = {\n",
        "        label: float(score)\n",
        "        for label, score in zip(labels, similarities)\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "classification = ZeroShortClassification()\n",
        "text = \"The new quantum computer can perform calculations in seconds that would take classical computers thousands of years.\"\n",
        "labels = [\"technology\", \"sports\", \"cooking\", \"politics\"]\n",
        "\n",
        "results = classification.classify(text, labels)\n",
        "print(\"Zero-shot classification results:\")\n",
        "for labels, scores in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f\"{labels}: {scores:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFfsbpB4-I_q",
        "outputId": "f891ea5f-2d6e-4a2f-c073-eae0dcd7b86e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This text is about label technology', 'This text is about label sports', 'This text is about label cooking', 'This text is about label politics']\n",
            "Zero-shot classification results:\n",
            "technology: 0.0904\n",
            "cooking: 0.0000\n",
            "politics: -0.0346\n",
            "sports: -0.0473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"seahawks is professional American football team based in Seattle\"\n",
        "labels = [\"technology\", \"sports\", \"cooking\", \"politics\"]\n",
        "results = classification.classify(text, labels)\n",
        "print(f\"Zero-shot classification results: {results}\")\n",
        "for labels, scores in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(f\"{labels}: {scores:.4f}\")\n"
      ],
      "metadata": {
        "id": "jNnTt1yFYzo3",
        "outputId": "2f967931-03a2-4ad7-803e-92c356cde572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This text is about label technology', 'This text is about label sports', 'This text is about label cooking', 'This text is about label politics']\n",
            "Zero-shot classification results: {'technology': 0.12781697511672974, 'sports': 0.31689268350601196, 'cooking': 0.11538080871105194, 'politics': 0.15189498662948608}\n",
            "sports: 0.3169\n",
            "politics: 0.1519\n",
            "technology: 0.1278\n",
            "cooking: 0.1154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how context affects embeddings\n",
        "texts = [\n",
        "        \"The bank by the river is been destroyed\",\n",
        "        \"The bank approved my loan\"\n",
        "]\n",
        "embeddings = model.encode(texts)"
      ],
      "metadata": {
        "id": "_AzldSzxZ3O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3te1DGtaahr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}